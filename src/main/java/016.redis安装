docker -run -d -p 6379:6379 --net=host redis:6.0.8

注释
	# bind 127.0.0.1
	daemonize no # 与docker run -d参数冲突
	持久化 appendonly yes

docker run -d -p 6379:6379 --name myre3 --privileged=true \
-v /opt/bigdata/redis/redis.conf:/etc/redis/redis.conf \
-v /opt/bigdata/redis/data:/data \
--net=host \
redis:6.0.8 redis-server /etc/redis/redis.conf
让redis读取容器内部修改过的配置


1、分布式落盘
	1.1 hash取余
		假设3台机器构成一个集群，用户每次读写操作都是根据公式hash(key)%N台机器，计算出hashcode，
		决定数据映射到哪一个节点上。

		缺点：原来规划好的节点，进行扩容或者缩容比较麻烦，因为hash取余后，节点变化，余数会变化。
		需要重新规划数据。

	1.2 一致性hash算法分区
		设计目的是为了解决分布式缓存的数据变动和映射问题，某个机器宕机了，分母变化，取余就有问题。

		构建一致性hash环
			一致性hash算法必然有个hash函数，并按照算法产生hash值，这个算法的所有可能hash值会构成一个全量集，
			这个集合可以成为一个hash空间[0,2^32-1],这是一个线性空间。在算法中，我盟通过适当的逻辑控制将它首尾相连，
			这样让它逻辑上形成一个环形空间。

			一致性hash算法是对2^32取模。

		服务器ip节点映射
			将集群中的各个ip节点映射到环上的某一个位置

		key落到服务器的落键规则
			存储kv时，计算key的hash值，将这个key使用相同的函数hash计算出hash值，并确定此数据在环上的位置，
			从此位置往环顺时针“行走”，第一台遇到的服务器就是其应该定位到的服务器，并将该键值对存储在该节点上。

		缺点：
			数据倾斜问题

		加入或者删除节点只影响相邻的一台机器

	1.3 hash槽分区
		hash槽实质就是一个数组，数组[0,2^14-1]形成hash slot空间。

		实际：用于解决均匀分配问题，在数据和节点之间又加入了一层，把这层称为hash槽（slot），用于管理数据和节点之间的关系。
		现在就相当于节点上放的是槽，槽里放的是数据。

		槽解决的是粒度问题，相当于把粒度变大了，这样便于数据移动。
		哈希解决的是映射问题，使用key的hash值来计算所在的槽，便于数据分配。

		一个集群只能由16384（2^14-1）个槽。这些槽会分配给集群中所有主节点，分配的策略没有要求。
		可以指定那些编号的槽分配给哪个主节点。集群会记录节点和槽的对应关系。解决了节点和槽的关系后，接下来就需要
		对key求hash值，然后对16384取余，余数就对应着槽。
		以槽为单位移动数据，因为槽的数目是固定的，处理起来比较容易，这样数据移动问题就解决了。

		16384：保证心跳方便和数据传输最大化。
			如果槽位为65536，发送心跳信息的消息头就达8k，发送的心跳过于庞大。

			在消息头中最占空间的是myslots[槽个数/8]。当槽位为65536时，这块大小为8k。
			因为redis节点需要发送一定数量的ping消息作为心跳包，如果槽位为65536，ping的消息会很大

		redis的集群主节点数量基本不可能达到1000个
			集群节点越多，心跳包的消息体内携带的数据越多。超过1000个，会导致网络阻塞

		槽位越小，节点少的情况下，压缩比高，容易传输。
			Redis主节点的配置信息中它所负责的hash槽是通过一张bitmap的形式来保存的，在传输过程中对bitmap进行压缩，
			但是如果bitmap的填充率slots/N很高的话，bitmap的压缩率就很低，如果节点数很少，而hash槽数量很多的话，bitmap的压缩率就很低。


		redis集群中内置了16384个hash槽，redis会根据节点数量大致均等的将hash槽映射到不同的节点。
		当需要在Redis集群放置一个key-value时，先对key使用crc16算法算出一个结果，然后对16384求余数，

		redis1：0-5460
		redis2：5461-10922
		redis3：10923-16383